
#newwts = [wt - eta * cg for wt, cg in zip(wts, cgs)]

'''
Nodes gives an output through an automated way
XOR multi layered (we were just working with step functions
AHHHHHHHHHHHHHHHHHHH)
delta rule one layer one weight
rate of change - uses the derative d/dx

differnaite e = e

def sigma(product):
return 1/exp - product

deltaerror/delta weights
error = desired - observed (sigma(product))

(np.dot(inputs, weights))

outputs takes a vector give 1 2 1 and gives it a number
cost(weight)
    has a sigmoid threshold activation and it is a function of the weights
unwrap the functino to make it

have sub script for neuron ith neuron to talk to the kth inout


use super script and if you want to find on the L (B,A prior) need to get
the sigmoid activation which'll be the weights connecting to layers
    wants the jth neuron

the lines  is the norm which is the L2 norm of lke summation squaring everything

global costs  = Compute the weighted cost function error vector for the last layer.

#each input has a specifc output for every element

Built weight matrix and it runs

partial deratiive f (x,y) change the output as a function of the x or as a function of the y
    change in the output is attribuited to change in one of the output (curly d)

the neuron that connects of i and j then cost function change a sa result of changing
only one

Second step of the notebook

Cost function is dependent on all of the weight
    if we know the cost function

y hat is the estimate of the mean

running the input it has the threshold and it tells us what the output  is

y - y hat is the sme as the cost function just didn't writing
    take dependent deratiive of x but x depends on z
    chain rlue take deraitive of x
i know y hat depends on estimate and y hat depends on the weight
    y hat is the sigma of activation and is dependent on the weights

prime = deraitve

>.5 then 1 else 0

ask him why he's transposing and this   newwts = [wt -eta*cg for wt,cg in zip(wts,cgs)]
    this question was answered because he's just doing the update function within a single line lol



'''